{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 396 images belonging to 3 classes.\n",
      "Found 264 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 185, 185\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_2_model.h5'\n",
    "train_data_dir = 'Posters 660/Train'\n",
    "validation_data_dir = 'Posters 660/Validation'\n",
    "nb_train_samples = 396\n",
    "nb_validation_samples = 264\n",
    "epochs = 50\n",
    "batch_size = 36 # difference in this version: train the VGG using a larger batch size\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_2_train.npy', 'w'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_2_validation.npy', 'w'),\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 264 samples\n",
      "Epoch 1/125\n",
      "396/396 [==============================] - 1s - loss: 0.7035 - acc: 0.5076 - val_loss: 0.6913 - val_acc: 0.5265\n",
      "Epoch 2/125\n",
      "396/396 [==============================] - 0s - loss: 0.6955 - acc: 0.5025 - val_loss: 0.6908 - val_acc: 0.5417\n",
      "Epoch 3/125\n",
      "396/396 [==============================] - 0s - loss: 0.6967 - acc: 0.4949 - val_loss: 0.6903 - val_acc: 0.5644\n",
      "Epoch 4/125\n",
      "396/396 [==============================] - 0s - loss: 0.6873 - acc: 0.5556 - val_loss: 0.6902 - val_acc: 0.5568\n",
      "Epoch 5/125\n",
      "396/396 [==============================] - 0s - loss: 0.6934 - acc: 0.4975 - val_loss: 0.6898 - val_acc: 0.5720\n",
      "Epoch 6/125\n",
      "396/396 [==============================] - 0s - loss: 0.6910 - acc: 0.5202 - val_loss: 0.6892 - val_acc: 0.5644\n",
      "Epoch 7/125\n",
      "396/396 [==============================] - 0s - loss: 0.6926 - acc: 0.5126 - val_loss: 0.6888 - val_acc: 0.5682\n",
      "Epoch 8/125\n",
      "396/396 [==============================] - 0s - loss: 0.6909 - acc: 0.5530 - val_loss: 0.6887 - val_acc: 0.5682\n",
      "Epoch 9/125\n",
      "396/396 [==============================] - 0s - loss: 0.6931 - acc: 0.5000 - val_loss: 0.6885 - val_acc: 0.5795\n",
      "Epoch 10/125\n",
      "396/396 [==============================] - 0s - loss: 0.6873 - acc: 0.5429 - val_loss: 0.6881 - val_acc: 0.5871\n",
      "Epoch 11/125\n",
      "396/396 [==============================] - 0s - loss: 0.6895 - acc: 0.5379 - val_loss: 0.6878 - val_acc: 0.5758\n",
      "Epoch 12/125\n",
      "396/396 [==============================] - 0s - loss: 0.6844 - acc: 0.5581 - val_loss: 0.6872 - val_acc: 0.5720\n",
      "Epoch 13/125\n",
      "396/396 [==============================] - 0s - loss: 0.6849 - acc: 0.5556 - val_loss: 0.6868 - val_acc: 0.5833\n",
      "Epoch 14/125\n",
      "396/396 [==============================] - 0s - loss: 0.6864 - acc: 0.5404 - val_loss: 0.6867 - val_acc: 0.5833\n",
      "Epoch 15/125\n",
      "396/396 [==============================] - 0s - loss: 0.6768 - acc: 0.6263 - val_loss: 0.6858 - val_acc: 0.6061\n",
      "Epoch 16/125\n",
      "396/396 [==============================] - 0s - loss: 0.6864 - acc: 0.5657 - val_loss: 0.6854 - val_acc: 0.6023\n",
      "Epoch 17/125\n",
      "396/396 [==============================] - 0s - loss: 0.6816 - acc: 0.5934 - val_loss: 0.6852 - val_acc: 0.5947\n",
      "Epoch 18/125\n",
      "396/396 [==============================] - 0s - loss: 0.6882 - acc: 0.5606 - val_loss: 0.6848 - val_acc: 0.5947\n",
      "Epoch 19/125\n",
      "396/396 [==============================] - 0s - loss: 0.6778 - acc: 0.5985 - val_loss: 0.6843 - val_acc: 0.6023\n",
      "Epoch 20/125\n",
      "396/396 [==============================] - 0s - loss: 0.6783 - acc: 0.6162 - val_loss: 0.6840 - val_acc: 0.5985\n",
      "Epoch 21/125\n",
      "396/396 [==============================] - 0s - loss: 0.6777 - acc: 0.5682 - val_loss: 0.6830 - val_acc: 0.6174\n",
      "Epoch 22/125\n",
      "396/396 [==============================] - 0s - loss: 0.6770 - acc: 0.5909 - val_loss: 0.6823 - val_acc: 0.6061\n",
      "Epoch 23/125\n",
      "396/396 [==============================] - 0s - loss: 0.6810 - acc: 0.5934 - val_loss: 0.6818 - val_acc: 0.6250\n",
      "Epoch 24/125\n",
      "396/396 [==============================] - 0s - loss: 0.6786 - acc: 0.5960 - val_loss: 0.6809 - val_acc: 0.6136\n",
      "Epoch 25/125\n",
      "396/396 [==============================] - 0s - loss: 0.6825 - acc: 0.5758 - val_loss: 0.6810 - val_acc: 0.6136\n",
      "Epoch 26/125\n",
      "396/396 [==============================] - 0s - loss: 0.6805 - acc: 0.5859 - val_loss: 0.6798 - val_acc: 0.6288\n",
      "Epoch 27/125\n",
      "396/396 [==============================] - 0s - loss: 0.6705 - acc: 0.6288 - val_loss: 0.6790 - val_acc: 0.6288\n",
      "Epoch 28/125\n",
      "396/396 [==============================] - 0s - loss: 0.6795 - acc: 0.6061 - val_loss: 0.6782 - val_acc: 0.6250\n",
      "Epoch 29/125\n",
      "396/396 [==============================] - 0s - loss: 0.6757 - acc: 0.6035 - val_loss: 0.6777 - val_acc: 0.6288\n",
      "Epoch 30/125\n",
      "396/396 [==============================] - 0s - loss: 0.6712 - acc: 0.6237 - val_loss: 0.6772 - val_acc: 0.6326\n",
      "Epoch 31/125\n",
      "396/396 [==============================] - 0s - loss: 0.6729 - acc: 0.6237 - val_loss: 0.6763 - val_acc: 0.6288\n",
      "Epoch 32/125\n",
      "396/396 [==============================] - 0s - loss: 0.6694 - acc: 0.6086 - val_loss: 0.6757 - val_acc: 0.6364\n",
      "Epoch 33/125\n",
      "396/396 [==============================] - 0s - loss: 0.6666 - acc: 0.6338 - val_loss: 0.6748 - val_acc: 0.6364\n",
      "Epoch 34/125\n",
      "396/396 [==============================] - 0s - loss: 0.6680 - acc: 0.6414 - val_loss: 0.6738 - val_acc: 0.6402\n",
      "Epoch 35/125\n",
      "396/396 [==============================] - 0s - loss: 0.6667 - acc: 0.6086 - val_loss: 0.6728 - val_acc: 0.6439\n",
      "Epoch 36/125\n",
      "396/396 [==============================] - 0s - loss: 0.6631 - acc: 0.6313 - val_loss: 0.6719 - val_acc: 0.6515\n",
      "Epoch 37/125\n",
      "396/396 [==============================] - 0s - loss: 0.6674 - acc: 0.6237 - val_loss: 0.6710 - val_acc: 0.6515\n",
      "Epoch 38/125\n",
      "396/396 [==============================] - 0s - loss: 0.6658 - acc: 0.6035 - val_loss: 0.6707 - val_acc: 0.6667\n",
      "Epoch 39/125\n",
      "396/396 [==============================] - 0s - loss: 0.6625 - acc: 0.6465 - val_loss: 0.6698 - val_acc: 0.6667\n",
      "Epoch 40/125\n",
      "396/396 [==============================] - 0s - loss: 0.6641 - acc: 0.6515 - val_loss: 0.6688 - val_acc: 0.6629\n",
      "Epoch 41/125\n",
      "396/396 [==============================] - 0s - loss: 0.6495 - acc: 0.6667 - val_loss: 0.6673 - val_acc: 0.6705\n",
      "Epoch 42/125\n",
      "396/396 [==============================] - 0s - loss: 0.6565 - acc: 0.6338 - val_loss: 0.6663 - val_acc: 0.6742\n",
      "Epoch 43/125\n",
      "396/396 [==============================] - 0s - loss: 0.6533 - acc: 0.6641 - val_loss: 0.6655 - val_acc: 0.6705\n",
      "Epoch 44/125\n",
      "396/396 [==============================] - 0s - loss: 0.6582 - acc: 0.6364 - val_loss: 0.6641 - val_acc: 0.6818\n",
      "Epoch 45/125\n",
      "396/396 [==============================] - 0s - loss: 0.6401 - acc: 0.7045 - val_loss: 0.6624 - val_acc: 0.6780\n",
      "Epoch 46/125\n",
      "396/396 [==============================] - 0s - loss: 0.6512 - acc: 0.6490 - val_loss: 0.6623 - val_acc: 0.6705\n",
      "Epoch 47/125\n",
      "396/396 [==============================] - 0s - loss: 0.6440 - acc: 0.6818 - val_loss: 0.6596 - val_acc: 0.6553\n",
      "Epoch 48/125\n",
      "396/396 [==============================] - 0s - loss: 0.6420 - acc: 0.6843 - val_loss: 0.6581 - val_acc: 0.6742\n",
      "Epoch 49/125\n",
      "396/396 [==============================] - 0s - loss: 0.6381 - acc: 0.6818 - val_loss: 0.6565 - val_acc: 0.6742\n",
      "Epoch 50/125\n",
      "396/396 [==============================] - 0s - loss: 0.6455 - acc: 0.6869 - val_loss: 0.6544 - val_acc: 0.6894\n",
      "Epoch 51/125\n",
      "396/396 [==============================] - 0s - loss: 0.6358 - acc: 0.6919 - val_loss: 0.6533 - val_acc: 0.6780\n",
      "Epoch 52/125\n",
      "396/396 [==============================] - 0s - loss: 0.6476 - acc: 0.6616 - val_loss: 0.6516 - val_acc: 0.6818\n",
      "Epoch 53/125\n",
      "396/396 [==============================] - 0s - loss: 0.6343 - acc: 0.7096 - val_loss: 0.6495 - val_acc: 0.6856\n",
      "Epoch 54/125\n",
      "396/396 [==============================] - 0s - loss: 0.6235 - acc: 0.6944 - val_loss: 0.6468 - val_acc: 0.6856\n",
      "Epoch 55/125\n",
      "396/396 [==============================] - 0s - loss: 0.6342 - acc: 0.6818 - val_loss: 0.6451 - val_acc: 0.6932\n",
      "Epoch 56/125\n",
      "396/396 [==============================] - 0s - loss: 0.6271 - acc: 0.6995 - val_loss: 0.6448 - val_acc: 0.7008\n",
      "Epoch 57/125\n",
      "396/396 [==============================] - 0s - loss: 0.6236 - acc: 0.6843 - val_loss: 0.6425 - val_acc: 0.6894\n",
      "Epoch 58/125\n",
      "396/396 [==============================] - 0s - loss: 0.6199 - acc: 0.7323 - val_loss: 0.6400 - val_acc: 0.6932\n",
      "Epoch 59/125\n",
      "396/396 [==============================] - 0s - loss: 0.6145 - acc: 0.7374 - val_loss: 0.6378 - val_acc: 0.7008\n",
      "Epoch 60/125\n",
      "396/396 [==============================] - 0s - loss: 0.6070 - acc: 0.7500 - val_loss: 0.6347 - val_acc: 0.6894\n",
      "Epoch 61/125\n",
      "396/396 [==============================] - 0s - loss: 0.6091 - acc: 0.7374 - val_loss: 0.6328 - val_acc: 0.6932\n",
      "Epoch 62/125\n",
      "396/396 [==============================] - 0s - loss: 0.6154 - acc: 0.6869 - val_loss: 0.6310 - val_acc: 0.6970\n",
      "Epoch 63/125\n",
      "396/396 [==============================] - 0s - loss: 0.6078 - acc: 0.7045 - val_loss: 0.6296 - val_acc: 0.6894\n",
      "Epoch 64/125\n",
      "396/396 [==============================] - 0s - loss: 0.5929 - acc: 0.7525 - val_loss: 0.6271 - val_acc: 0.6894\n",
      "Epoch 65/125\n",
      "396/396 [==============================] - 0s - loss: 0.5984 - acc: 0.7525 - val_loss: 0.6255 - val_acc: 0.6932\n",
      "Epoch 66/125\n",
      "396/396 [==============================] - 0s - loss: 0.5911 - acc: 0.7399 - val_loss: 0.6230 - val_acc: 0.6932\n",
      "Epoch 67/125\n",
      "396/396 [==============================] - 0s - loss: 0.5958 - acc: 0.7323 - val_loss: 0.6208 - val_acc: 0.6894\n",
      "Epoch 68/125\n",
      "396/396 [==============================] - 0s - loss: 0.5802 - acc: 0.7374 - val_loss: 0.6179 - val_acc: 0.6894\n",
      "Epoch 69/125\n",
      "396/396 [==============================] - 0s - loss: 0.5882 - acc: 0.7323 - val_loss: 0.6161 - val_acc: 0.6894\n",
      "Epoch 70/125\n",
      "396/396 [==============================] - 0s - loss: 0.5635 - acc: 0.7702 - val_loss: 0.6140 - val_acc: 0.6932\n",
      "Epoch 71/125\n",
      "396/396 [==============================] - 0s - loss: 0.5728 - acc: 0.7399 - val_loss: 0.6116 - val_acc: 0.6970\n",
      "Epoch 72/125\n",
      "396/396 [==============================] - 0s - loss: 0.5732 - acc: 0.7576 - val_loss: 0.6091 - val_acc: 0.6970\n",
      "Epoch 73/125\n",
      "396/396 [==============================] - 0s - loss: 0.5480 - acc: 0.7601 - val_loss: 0.6071 - val_acc: 0.7008\n",
      "Epoch 74/125\n",
      "396/396 [==============================] - 0s - loss: 0.5552 - acc: 0.7955 - val_loss: 0.6051 - val_acc: 0.6932\n",
      "Epoch 75/125\n",
      "396/396 [==============================] - 0s - loss: 0.5527 - acc: 0.7551 - val_loss: 0.6035 - val_acc: 0.6932\n",
      "Epoch 76/125\n",
      "396/396 [==============================] - 0s - loss: 0.5494 - acc: 0.7828 - val_loss: 0.6021 - val_acc: 0.6970\n",
      "Epoch 77/125\n",
      "396/396 [==============================] - 0s - loss: 0.5443 - acc: 0.7652 - val_loss: 0.5995 - val_acc: 0.6970\n",
      "Epoch 78/125\n",
      "396/396 [==============================] - 0s - loss: 0.5386 - acc: 0.7727 - val_loss: 0.5971 - val_acc: 0.7008\n",
      "Epoch 79/125\n",
      "396/396 [==============================] - 0s - loss: 0.5441 - acc: 0.7803 - val_loss: 0.5956 - val_acc: 0.7045\n",
      "Epoch 80/125\n",
      "396/396 [==============================] - 0s - loss: 0.5424 - acc: 0.7879 - val_loss: 0.5943 - val_acc: 0.6970\n",
      "Epoch 81/125\n",
      "396/396 [==============================] - 0s - loss: 0.5303 - acc: 0.7929 - val_loss: 0.5923 - val_acc: 0.7045\n",
      "Epoch 82/125\n",
      "396/396 [==============================] - 0s - loss: 0.5140 - acc: 0.8232 - val_loss: 0.5904 - val_acc: 0.6970\n",
      "Epoch 83/125\n",
      "396/396 [==============================] - 0s - loss: 0.5067 - acc: 0.8232 - val_loss: 0.5884 - val_acc: 0.7045\n",
      "Epoch 84/125\n",
      "396/396 [==============================] - 0s - loss: 0.5206 - acc: 0.8182 - val_loss: 0.5875 - val_acc: 0.7045\n",
      "Epoch 85/125\n",
      "396/396 [==============================] - 0s - loss: 0.4945 - acc: 0.8333 - val_loss: 0.5849 - val_acc: 0.7045\n",
      "Epoch 86/125\n",
      "396/396 [==============================] - 0s - loss: 0.4963 - acc: 0.8333 - val_loss: 0.5832 - val_acc: 0.7083\n",
      "Epoch 87/125\n",
      "396/396 [==============================] - 0s - loss: 0.5055 - acc: 0.8182 - val_loss: 0.5817 - val_acc: 0.7121\n",
      "Epoch 88/125\n",
      "396/396 [==============================] - 0s - loss: 0.4927 - acc: 0.8207 - val_loss: 0.5801 - val_acc: 0.7121\n",
      "Epoch 89/125\n",
      "396/396 [==============================] - 0s - loss: 0.4827 - acc: 0.8333 - val_loss: 0.5785 - val_acc: 0.7121\n",
      "Epoch 90/125\n",
      "396/396 [==============================] - 0s - loss: 0.4941 - acc: 0.8157 - val_loss: 0.5773 - val_acc: 0.7121\n",
      "Epoch 91/125\n",
      "396/396 [==============================] - 0s - loss: 0.4800 - acc: 0.8232 - val_loss: 0.5777 - val_acc: 0.7008\n",
      "Epoch 92/125\n",
      "396/396 [==============================] - 0s - loss: 0.4572 - acc: 0.8409 - val_loss: 0.5753 - val_acc: 0.7121\n",
      "Epoch 93/125\n",
      "396/396 [==============================] - 0s - loss: 0.4710 - acc: 0.8359 - val_loss: 0.5734 - val_acc: 0.7121\n",
      "Epoch 94/125\n",
      "396/396 [==============================] - 0s - loss: 0.4698 - acc: 0.8081 - val_loss: 0.5725 - val_acc: 0.7083\n",
      "Epoch 95/125\n",
      "396/396 [==============================] - 0s - loss: 0.4517 - acc: 0.8359 - val_loss: 0.5714 - val_acc: 0.7121\n",
      "Epoch 96/125\n",
      "396/396 [==============================] - 0s - loss: 0.4539 - acc: 0.8384 - val_loss: 0.5701 - val_acc: 0.7197\n",
      "Epoch 97/125\n",
      "396/396 [==============================] - 0s - loss: 0.4511 - acc: 0.8510 - val_loss: 0.5692 - val_acc: 0.7083\n",
      "Epoch 98/125\n",
      "396/396 [==============================] - 0s - loss: 0.4149 - acc: 0.8763 - val_loss: 0.5690 - val_acc: 0.7045\n",
      "Epoch 99/125\n",
      "396/396 [==============================] - 0s - loss: 0.4417 - acc: 0.8510 - val_loss: 0.5680 - val_acc: 0.7121\n",
      "Epoch 100/125\n",
      "396/396 [==============================] - 0s - loss: 0.4162 - acc: 0.8636 - val_loss: 0.5671 - val_acc: 0.7273\n",
      "Epoch 101/125\n",
      "396/396 [==============================] - 0s - loss: 0.4230 - acc: 0.8662 - val_loss: 0.5665 - val_acc: 0.7121\n",
      "Epoch 102/125\n",
      "396/396 [==============================] - 0s - loss: 0.4170 - acc: 0.8586 - val_loss: 0.5653 - val_acc: 0.7159\n",
      "Epoch 103/125\n",
      "396/396 [==============================] - 0s - loss: 0.4079 - acc: 0.8687 - val_loss: 0.5692 - val_acc: 0.6970\n",
      "Epoch 104/125\n",
      "396/396 [==============================] - 0s - loss: 0.4139 - acc: 0.8662 - val_loss: 0.5655 - val_acc: 0.7121\n",
      "Epoch 105/125\n",
      "396/396 [==============================] - 0s - loss: 0.4117 - acc: 0.8561 - val_loss: 0.5656 - val_acc: 0.6970\n",
      "Epoch 106/125\n",
      "396/396 [==============================] - 0s - loss: 0.3925 - acc: 0.8712 - val_loss: 0.5639 - val_acc: 0.7159\n",
      "Epoch 107/125\n",
      "396/396 [==============================] - 0s - loss: 0.3906 - acc: 0.8636 - val_loss: 0.5639 - val_acc: 0.7121\n",
      "Epoch 108/125\n",
      "396/396 [==============================] - 0s - loss: 0.3923 - acc: 0.8712 - val_loss: 0.5635 - val_acc: 0.7121\n",
      "Epoch 109/125\n",
      "396/396 [==============================] - 0s - loss: 0.3852 - acc: 0.8687 - val_loss: 0.5633 - val_acc: 0.7235\n",
      "Epoch 110/125\n",
      "396/396 [==============================] - 0s - loss: 0.3761 - acc: 0.8737 - val_loss: 0.5637 - val_acc: 0.7083\n",
      "Epoch 111/125\n",
      "396/396 [==============================] - 0s - loss: 0.3779 - acc: 0.8813 - val_loss: 0.5642 - val_acc: 0.7235\n",
      "Epoch 112/125\n",
      "396/396 [==============================] - 0s - loss: 0.3682 - acc: 0.8864 - val_loss: 0.5639 - val_acc: 0.7159\n",
      "Epoch 113/125\n",
      "396/396 [==============================] - 0s - loss: 0.3726 - acc: 0.8864 - val_loss: 0.5637 - val_acc: 0.7159\n",
      "Epoch 114/125\n",
      "396/396 [==============================] - 0s - loss: 0.3600 - acc: 0.8889 - val_loss: 0.5645 - val_acc: 0.7159\n",
      "Epoch 115/125\n",
      "396/396 [==============================] - 0s - loss: 0.3639 - acc: 0.8965 - val_loss: 0.5643 - val_acc: 0.7045\n",
      "Epoch 116/125\n",
      "396/396 [==============================] - 0s - loss: 0.3435 - acc: 0.8990 - val_loss: 0.5642 - val_acc: 0.7197\n",
      "Epoch 117/125\n",
      "396/396 [==============================] - 0s - loss: 0.3430 - acc: 0.8965 - val_loss: 0.5665 - val_acc: 0.7121\n",
      "Epoch 118/125\n",
      "396/396 [==============================] - 0s - loss: 0.3410 - acc: 0.8990 - val_loss: 0.5647 - val_acc: 0.7008\n",
      "Epoch 119/125\n",
      "396/396 [==============================] - 0s - loss: 0.3258 - acc: 0.9217 - val_loss: 0.5652 - val_acc: 0.7008\n",
      "Epoch 120/125\n",
      "396/396 [==============================] - 0s - loss: 0.3336 - acc: 0.9116 - val_loss: 0.5667 - val_acc: 0.7159\n",
      "Epoch 121/125\n",
      "396/396 [==============================] - 0s - loss: 0.3238 - acc: 0.9141 - val_loss: 0.5688 - val_acc: 0.6970\n",
      "Epoch 122/125\n",
      "396/396 [==============================] - 0s - loss: 0.3401 - acc: 0.8939 - val_loss: 0.5693 - val_acc: 0.7121\n",
      "Epoch 123/125\n",
      "396/396 [==============================] - 0s - loss: 0.3115 - acc: 0.9066 - val_loss: 0.5669 - val_acc: 0.7083\n",
      "Epoch 124/125\n",
      "396/396 [==============================] - 0s - loss: 0.3182 - acc: 0.9091 - val_loss: 0.5676 - val_acc: 0.7045\n",
      "Epoch 125/125\n",
      "396/396 [==============================] - 0s - loss: 0.3080 - acc: 0.9116 - val_loss: 0.5689 - val_acc: 0.7121\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'Posters 660/Train'\n",
    "validation_data_dir = 'Posters 660/Validation'\n",
    "nb_train_samples = 396\n",
    "nb_validation_samples = 264\n",
    "epochs = 125\n",
    "batch_size = 36 \n",
    "\n",
    "sgd = SGD(lr=0.0001, momentum=0.9) # lower learning rate\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples / 2) + [1] * (nb_train_samples / 2))\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples / 2) + [1] * (nb_validation_samples / 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "#add more layers, eliminate dropout\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu')) \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 264 samples\n",
      "Epoch 1/125\n",
      "396/396 [==============================] - 1s - loss: 0.7035 - acc: 0.4874 - val_loss: 0.6985 - val_acc: 0.5000\n",
      "Epoch 2/125\n",
      "396/396 [==============================] - 0s - loss: 0.7011 - acc: 0.4747 - val_loss: 0.6975 - val_acc: 0.4924\n",
      "Epoch 3/125\n",
      "396/396 [==============================] - 0s - loss: 0.6987 - acc: 0.4697 - val_loss: 0.6967 - val_acc: 0.4924\n",
      "Epoch 4/125\n",
      "396/396 [==============================] - 0s - loss: 0.6938 - acc: 0.4899 - val_loss: 0.6961 - val_acc: 0.4886\n",
      "Epoch 5/125\n",
      "396/396 [==============================] - 0s - loss: 0.6963 - acc: 0.5025 - val_loss: 0.6955 - val_acc: 0.4962\n",
      "Epoch 6/125\n",
      "396/396 [==============================] - 0s - loss: 0.7002 - acc: 0.4798 - val_loss: 0.6945 - val_acc: 0.4886\n",
      "Epoch 7/125\n",
      "396/396 [==============================] - 0s - loss: 0.6957 - acc: 0.4798 - val_loss: 0.6933 - val_acc: 0.4886\n",
      "Epoch 8/125\n",
      "396/396 [==============================] - 0s - loss: 0.6935 - acc: 0.4924 - val_loss: 0.6927 - val_acc: 0.4962\n",
      "Epoch 9/125\n",
      "396/396 [==============================] - 0s - loss: 0.6871 - acc: 0.5556 - val_loss: 0.6923 - val_acc: 0.4924\n",
      "Epoch 10/125\n",
      "396/396 [==============================] - 0s - loss: 0.6896 - acc: 0.4899 - val_loss: 0.6919 - val_acc: 0.5000\n",
      "Epoch 11/125\n",
      "396/396 [==============================] - 0s - loss: 0.6858 - acc: 0.5707 - val_loss: 0.6917 - val_acc: 0.4962\n",
      "Epoch 12/125\n",
      "396/396 [==============================] - 0s - loss: 0.6951 - acc: 0.5278 - val_loss: 0.6914 - val_acc: 0.4962\n",
      "Epoch 13/125\n",
      "396/396 [==============================] - 0s - loss: 0.6943 - acc: 0.5076 - val_loss: 0.6914 - val_acc: 0.4962\n",
      "Epoch 14/125\n",
      "396/396 [==============================] - 0s - loss: 0.6899 - acc: 0.5278 - val_loss: 0.6909 - val_acc: 0.5000\n",
      "Epoch 15/125\n",
      "396/396 [==============================] - 0s - loss: 0.6884 - acc: 0.5455 - val_loss: 0.6903 - val_acc: 0.5076\n",
      "Epoch 16/125\n",
      "396/396 [==============================] - 0s - loss: 0.6873 - acc: 0.5404 - val_loss: 0.6894 - val_acc: 0.5227\n",
      "Epoch 17/125\n",
      "396/396 [==============================] - 0s - loss: 0.6931 - acc: 0.5126 - val_loss: 0.6888 - val_acc: 0.5114\n",
      "Epoch 18/125\n",
      "396/396 [==============================] - 0s - loss: 0.6851 - acc: 0.5631 - val_loss: 0.6879 - val_acc: 0.5455\n",
      "Epoch 19/125\n",
      "396/396 [==============================] - 0s - loss: 0.6809 - acc: 0.6035 - val_loss: 0.6872 - val_acc: 0.5606\n",
      "Epoch 20/125\n",
      "396/396 [==============================] - 0s - loss: 0.6828 - acc: 0.5732 - val_loss: 0.6866 - val_acc: 0.5568\n",
      "Epoch 21/125\n",
      "396/396 [==============================] - 0s - loss: 0.6853 - acc: 0.5783 - val_loss: 0.6860 - val_acc: 0.5644\n",
      "Epoch 22/125\n",
      "396/396 [==============================] - 0s - loss: 0.6815 - acc: 0.5732 - val_loss: 0.6855 - val_acc: 0.5682\n",
      "Epoch 23/125\n",
      "396/396 [==============================] - 0s - loss: 0.6817 - acc: 0.5682 - val_loss: 0.6845 - val_acc: 0.5606\n",
      "Epoch 24/125\n",
      "396/396 [==============================] - 0s - loss: 0.6808 - acc: 0.5859 - val_loss: 0.6839 - val_acc: 0.5795\n",
      "Epoch 25/125\n",
      "396/396 [==============================] - 0s - loss: 0.6851 - acc: 0.5455 - val_loss: 0.6830 - val_acc: 0.5795\n",
      "Epoch 26/125\n",
      "396/396 [==============================] - 0s - loss: 0.6808 - acc: 0.5758 - val_loss: 0.6821 - val_acc: 0.5720\n",
      "Epoch 27/125\n",
      "396/396 [==============================] - 0s - loss: 0.6822 - acc: 0.5581 - val_loss: 0.6814 - val_acc: 0.5720\n",
      "Epoch 28/125\n",
      "396/396 [==============================] - 0s - loss: 0.6820 - acc: 0.5581 - val_loss: 0.6810 - val_acc: 0.5720\n",
      "Epoch 29/125\n",
      "396/396 [==============================] - 0s - loss: 0.6730 - acc: 0.6237 - val_loss: 0.6805 - val_acc: 0.5682\n",
      "Epoch 30/125\n",
      "396/396 [==============================] - 0s - loss: 0.6757 - acc: 0.5859 - val_loss: 0.6797 - val_acc: 0.5720\n",
      "Epoch 31/125\n",
      "396/396 [==============================] - 0s - loss: 0.6684 - acc: 0.6162 - val_loss: 0.6788 - val_acc: 0.5833\n",
      "Epoch 32/125\n",
      "396/396 [==============================] - 0s - loss: 0.6770 - acc: 0.6237 - val_loss: 0.6779 - val_acc: 0.5909\n",
      "Epoch 33/125\n",
      "396/396 [==============================] - 0s - loss: 0.6746 - acc: 0.5934 - val_loss: 0.6771 - val_acc: 0.5985\n",
      "Epoch 34/125\n",
      "396/396 [==============================] - 0s - loss: 0.6746 - acc: 0.5909 - val_loss: 0.6763 - val_acc: 0.6061\n",
      "Epoch 35/125\n",
      "396/396 [==============================] - 0s - loss: 0.6594 - acc: 0.6515 - val_loss: 0.6751 - val_acc: 0.6098\n",
      "Epoch 36/125\n",
      "396/396 [==============================] - 0s - loss: 0.6684 - acc: 0.6212 - val_loss: 0.6744 - val_acc: 0.6098\n",
      "Epoch 37/125\n",
      "396/396 [==============================] - 0s - loss: 0.6642 - acc: 0.6086 - val_loss: 0.6735 - val_acc: 0.6136\n",
      "Epoch 38/125\n",
      "396/396 [==============================] - 0s - loss: 0.6748 - acc: 0.6010 - val_loss: 0.6727 - val_acc: 0.6364\n",
      "Epoch 39/125\n",
      "396/396 [==============================] - 0s - loss: 0.6636 - acc: 0.6465 - val_loss: 0.6718 - val_acc: 0.6439\n",
      "Epoch 40/125\n",
      "396/396 [==============================] - 0s - loss: 0.6596 - acc: 0.6566 - val_loss: 0.6708 - val_acc: 0.6402\n",
      "Epoch 41/125\n",
      "396/396 [==============================] - 0s - loss: 0.6675 - acc: 0.6010 - val_loss: 0.6698 - val_acc: 0.6364\n",
      "Epoch 42/125\n",
      "396/396 [==============================] - 0s - loss: 0.6565 - acc: 0.6667 - val_loss: 0.6685 - val_acc: 0.6364\n",
      "Epoch 43/125\n",
      "396/396 [==============================] - 0s - loss: 0.6596 - acc: 0.6389 - val_loss: 0.6676 - val_acc: 0.6553\n",
      "Epoch 44/125\n",
      "396/396 [==============================] - 0s - loss: 0.6620 - acc: 0.6263 - val_loss: 0.6668 - val_acc: 0.6364\n",
      "Epoch 45/125\n",
      "396/396 [==============================] - 0s - loss: 0.6566 - acc: 0.6616 - val_loss: 0.6657 - val_acc: 0.6515\n",
      "Epoch 46/125\n",
      "396/396 [==============================] - 0s - loss: 0.6533 - acc: 0.6641 - val_loss: 0.6645 - val_acc: 0.6553\n",
      "Epoch 47/125\n",
      "396/396 [==============================] - 0s - loss: 0.6484 - acc: 0.6818 - val_loss: 0.6635 - val_acc: 0.6591\n",
      "Epoch 48/125\n",
      "396/396 [==============================] - 0s - loss: 0.6533 - acc: 0.6263 - val_loss: 0.6619 - val_acc: 0.6667\n",
      "Epoch 49/125\n",
      "396/396 [==============================] - 0s - loss: 0.6489 - acc: 0.6843 - val_loss: 0.6609 - val_acc: 0.6667\n",
      "Epoch 50/125\n",
      "396/396 [==============================] - 0s - loss: 0.6513 - acc: 0.6566 - val_loss: 0.6602 - val_acc: 0.6667\n",
      "Epoch 51/125\n",
      "396/396 [==============================] - 0s - loss: 0.6481 - acc: 0.6793 - val_loss: 0.6588 - val_acc: 0.6742\n",
      "Epoch 52/125\n",
      "396/396 [==============================] - 0s - loss: 0.6463 - acc: 0.6793 - val_loss: 0.6578 - val_acc: 0.6742\n",
      "Epoch 53/125\n",
      "396/396 [==============================] - 0s - loss: 0.6464 - acc: 0.6742 - val_loss: 0.6559 - val_acc: 0.6780\n",
      "Epoch 54/125\n",
      "396/396 [==============================] - 0s - loss: 0.6460 - acc: 0.6869 - val_loss: 0.6550 - val_acc: 0.6780\n",
      "Epoch 55/125\n",
      "396/396 [==============================] - 0s - loss: 0.6393 - acc: 0.6869 - val_loss: 0.6534 - val_acc: 0.6856\n",
      "Epoch 56/125\n",
      "396/396 [==============================] - 0s - loss: 0.6448 - acc: 0.6667 - val_loss: 0.6524 - val_acc: 0.6818\n",
      "Epoch 57/125\n",
      "396/396 [==============================] - 0s - loss: 0.6305 - acc: 0.6970 - val_loss: 0.6520 - val_acc: 0.6515\n",
      "Epoch 58/125\n",
      "396/396 [==============================] - 0s - loss: 0.6305 - acc: 0.7273 - val_loss: 0.6497 - val_acc: 0.6780\n",
      "Epoch 59/125\n",
      "396/396 [==============================] - 0s - loss: 0.6375 - acc: 0.6944 - val_loss: 0.6485 - val_acc: 0.6856\n",
      "Epoch 60/125\n",
      "396/396 [==============================] - 0s - loss: 0.6266 - acc: 0.6869 - val_loss: 0.6463 - val_acc: 0.6856\n",
      "Epoch 61/125\n",
      "396/396 [==============================] - 0s - loss: 0.6289 - acc: 0.6843 - val_loss: 0.6451 - val_acc: 0.6856\n",
      "Epoch 62/125\n",
      "396/396 [==============================] - 0s - loss: 0.6182 - acc: 0.7172 - val_loss: 0.6431 - val_acc: 0.6818\n",
      "Epoch 63/125\n",
      "396/396 [==============================] - 0s - loss: 0.6196 - acc: 0.7045 - val_loss: 0.6413 - val_acc: 0.6932\n",
      "Epoch 64/125\n",
      "396/396 [==============================] - 0s - loss: 0.6188 - acc: 0.6843 - val_loss: 0.6396 - val_acc: 0.7045\n",
      "Epoch 65/125\n",
      "396/396 [==============================] - 0s - loss: 0.6124 - acc: 0.7449 - val_loss: 0.6378 - val_acc: 0.6894\n",
      "Epoch 66/125\n",
      "396/396 [==============================] - 0s - loss: 0.6075 - acc: 0.7273 - val_loss: 0.6378 - val_acc: 0.6818\n",
      "Epoch 67/125\n",
      "396/396 [==============================] - 0s - loss: 0.6058 - acc: 0.7247 - val_loss: 0.6340 - val_acc: 0.7045\n",
      "Epoch 68/125\n",
      "396/396 [==============================] - 0s - loss: 0.6020 - acc: 0.7399 - val_loss: 0.6327 - val_acc: 0.6856\n",
      "Epoch 69/125\n",
      "396/396 [==============================] - 0s - loss: 0.6033 - acc: 0.7222 - val_loss: 0.6307 - val_acc: 0.6894\n",
      "Epoch 70/125\n",
      "396/396 [==============================] - 0s - loss: 0.5878 - acc: 0.7500 - val_loss: 0.6278 - val_acc: 0.6970\n",
      "Epoch 71/125\n",
      "396/396 [==============================] - 0s - loss: 0.5993 - acc: 0.7172 - val_loss: 0.6268 - val_acc: 0.6970\n",
      "Epoch 72/125\n",
      "396/396 [==============================] - 0s - loss: 0.5961 - acc: 0.7449 - val_loss: 0.6263 - val_acc: 0.6818\n",
      "Epoch 73/125\n",
      "396/396 [==============================] - 0s - loss: 0.5905 - acc: 0.7222 - val_loss: 0.6244 - val_acc: 0.6932\n",
      "Epoch 74/125\n",
      "396/396 [==============================] - 0s - loss: 0.5788 - acc: 0.7500 - val_loss: 0.6206 - val_acc: 0.7083\n",
      "Epoch 75/125\n",
      "396/396 [==============================] - 0s - loss: 0.5828 - acc: 0.7601 - val_loss: 0.6189 - val_acc: 0.6970\n",
      "Epoch 76/125\n",
      "396/396 [==============================] - 0s - loss: 0.5838 - acc: 0.7374 - val_loss: 0.6174 - val_acc: 0.7008\n",
      "Epoch 77/125\n",
      "396/396 [==============================] - 0s - loss: 0.5570 - acc: 0.7727 - val_loss: 0.6151 - val_acc: 0.7083\n",
      "Epoch 78/125\n",
      "396/396 [==============================] - 0s - loss: 0.5794 - acc: 0.7348 - val_loss: 0.6130 - val_acc: 0.6970\n",
      "Epoch 79/125\n",
      "396/396 [==============================] - 0s - loss: 0.5490 - acc: 0.7525 - val_loss: 0.6127 - val_acc: 0.6818\n",
      "Epoch 80/125\n",
      "396/396 [==============================] - 0s - loss: 0.5608 - acc: 0.7525 - val_loss: 0.6080 - val_acc: 0.7159\n",
      "Epoch 81/125\n",
      "396/396 [==============================] - 0s - loss: 0.5562 - acc: 0.7500 - val_loss: 0.6088 - val_acc: 0.7008\n",
      "Epoch 82/125\n",
      "396/396 [==============================] - 0s - loss: 0.5539 - acc: 0.7828 - val_loss: 0.6050 - val_acc: 0.7197\n",
      "Epoch 83/125\n",
      "396/396 [==============================] - 0s - loss: 0.5421 - acc: 0.7854 - val_loss: 0.6037 - val_acc: 0.7083\n",
      "Epoch 84/125\n",
      "396/396 [==============================] - 0s - loss: 0.5429 - acc: 0.7879 - val_loss: 0.6006 - val_acc: 0.7197\n",
      "Epoch 85/125\n",
      "396/396 [==============================] - 0s - loss: 0.5436 - acc: 0.7803 - val_loss: 0.6010 - val_acc: 0.7045\n",
      "Epoch 86/125\n",
      "396/396 [==============================] - 0s - loss: 0.5445 - acc: 0.7854 - val_loss: 0.5981 - val_acc: 0.7159\n",
      "Epoch 87/125\n",
      "396/396 [==============================] - 0s - loss: 0.5450 - acc: 0.7727 - val_loss: 0.5961 - val_acc: 0.7159\n",
      "Epoch 88/125\n",
      "396/396 [==============================] - 0s - loss: 0.5267 - acc: 0.8131 - val_loss: 0.5944 - val_acc: 0.7008\n",
      "Epoch 89/125\n",
      "396/396 [==============================] - 0s - loss: 0.5144 - acc: 0.7955 - val_loss: 0.5919 - val_acc: 0.7311\n",
      "Epoch 90/125\n",
      "396/396 [==============================] - 0s - loss: 0.5355 - acc: 0.7727 - val_loss: 0.5919 - val_acc: 0.7045\n",
      "Epoch 91/125\n",
      "396/396 [==============================] - 0s - loss: 0.5166 - acc: 0.8157 - val_loss: 0.5903 - val_acc: 0.7045\n",
      "Epoch 92/125\n",
      "396/396 [==============================] - 0s - loss: 0.5146 - acc: 0.8081 - val_loss: 0.5884 - val_acc: 0.7273\n",
      "Epoch 93/125\n",
      "396/396 [==============================] - 0s - loss: 0.5034 - acc: 0.7929 - val_loss: 0.5878 - val_acc: 0.7008\n",
      "Epoch 94/125\n",
      "396/396 [==============================] - 0s - loss: 0.4964 - acc: 0.8005 - val_loss: 0.5845 - val_acc: 0.7424\n",
      "Epoch 95/125\n",
      "396/396 [==============================] - 0s - loss: 0.4877 - acc: 0.8182 - val_loss: 0.5836 - val_acc: 0.7121\n",
      "Epoch 96/125\n",
      "396/396 [==============================] - 0s - loss: 0.5038 - acc: 0.7980 - val_loss: 0.5824 - val_acc: 0.7083\n",
      "Epoch 97/125\n",
      "396/396 [==============================] - 0s - loss: 0.4897 - acc: 0.8333 - val_loss: 0.5795 - val_acc: 0.7462\n",
      "Epoch 98/125\n",
      "396/396 [==============================] - 0s - loss: 0.4773 - acc: 0.8308 - val_loss: 0.5802 - val_acc: 0.7121\n",
      "Epoch 99/125\n",
      "396/396 [==============================] - 0s - loss: 0.4849 - acc: 0.8283 - val_loss: 0.5792 - val_acc: 0.7121\n",
      "Epoch 100/125\n",
      "396/396 [==============================] - 0s - loss: 0.4750 - acc: 0.8308 - val_loss: 0.5770 - val_acc: 0.7462\n",
      "Epoch 101/125\n",
      "396/396 [==============================] - 0s - loss: 0.4671 - acc: 0.8258 - val_loss: 0.5766 - val_acc: 0.7083\n",
      "Epoch 102/125\n",
      "396/396 [==============================] - 0s - loss: 0.4597 - acc: 0.8359 - val_loss: 0.5749 - val_acc: 0.7197\n",
      "Epoch 103/125\n",
      "396/396 [==============================] - 0s - loss: 0.4549 - acc: 0.8434 - val_loss: 0.5739 - val_acc: 0.7197\n",
      "Epoch 104/125\n",
      "396/396 [==============================] - 0s - loss: 0.4579 - acc: 0.8359 - val_loss: 0.5731 - val_acc: 0.7197\n",
      "Epoch 105/125\n",
      "396/396 [==============================] - 0s - loss: 0.4336 - acc: 0.8535 - val_loss: 0.5721 - val_acc: 0.7424\n",
      "Epoch 106/125\n",
      "396/396 [==============================] - 0s - loss: 0.4489 - acc: 0.8485 - val_loss: 0.5718 - val_acc: 0.7121\n",
      "Epoch 107/125\n",
      "396/396 [==============================] - 0s - loss: 0.4413 - acc: 0.8485 - val_loss: 0.5706 - val_acc: 0.7121\n",
      "Epoch 108/125\n",
      "396/396 [==============================] - 0s - loss: 0.4353 - acc: 0.8409 - val_loss: 0.5713 - val_acc: 0.7083\n",
      "Epoch 109/125\n",
      "396/396 [==============================] - 0s - loss: 0.4453 - acc: 0.8485 - val_loss: 0.5687 - val_acc: 0.7462\n",
      "Epoch 110/125\n",
      "396/396 [==============================] - 0s - loss: 0.4267 - acc: 0.8510 - val_loss: 0.5684 - val_acc: 0.7311\n",
      "Epoch 111/125\n",
      "396/396 [==============================] - 0s - loss: 0.4319 - acc: 0.8485 - val_loss: 0.5676 - val_acc: 0.7311\n",
      "Epoch 112/125\n",
      "396/396 [==============================] - 0s - loss: 0.4204 - acc: 0.8561 - val_loss: 0.5673 - val_acc: 0.7462\n",
      "Epoch 113/125\n",
      "396/396 [==============================] - 0s - loss: 0.4093 - acc: 0.8485 - val_loss: 0.5692 - val_acc: 0.7159\n",
      "Epoch 114/125\n",
      "396/396 [==============================] - 0s - loss: 0.4106 - acc: 0.8510 - val_loss: 0.5665 - val_acc: 0.7386\n",
      "Epoch 115/125\n",
      "396/396 [==============================] - 0s - loss: 0.4151 - acc: 0.8460 - val_loss: 0.5667 - val_acc: 0.7348\n",
      "Epoch 116/125\n",
      "396/396 [==============================] - 0s - loss: 0.4012 - acc: 0.8662 - val_loss: 0.5658 - val_acc: 0.7462\n",
      "Epoch 117/125\n",
      "396/396 [==============================] - 0s - loss: 0.3918 - acc: 0.8535 - val_loss: 0.5654 - val_acc: 0.7311\n",
      "Epoch 118/125\n",
      "396/396 [==============================] - 0s - loss: 0.3933 - acc: 0.8687 - val_loss: 0.5657 - val_acc: 0.7424\n",
      "Epoch 119/125\n",
      "396/396 [==============================] - 0s - loss: 0.3884 - acc: 0.8636 - val_loss: 0.5657 - val_acc: 0.7424\n",
      "Epoch 120/125\n",
      "396/396 [==============================] - 0s - loss: 0.3702 - acc: 0.8914 - val_loss: 0.5665 - val_acc: 0.7197\n",
      "Epoch 121/125\n",
      "396/396 [==============================] - 0s - loss: 0.3742 - acc: 0.8712 - val_loss: 0.5657 - val_acc: 0.7235\n",
      "Epoch 122/125\n",
      "396/396 [==============================] - 0s - loss: 0.3807 - acc: 0.8636 - val_loss: 0.5667 - val_acc: 0.7235\n",
      "Epoch 123/125\n",
      "396/396 [==============================] - 0s - loss: 0.3659 - acc: 0.8864 - val_loss: 0.5688 - val_acc: 0.7197\n",
      "Epoch 124/125\n",
      "396/396 [==============================] - 0s - loss: 0.3637 - acc: 0.8763 - val_loss: 0.5664 - val_acc: 0.7348\n",
      "Epoch 125/125\n",
      "396/396 [==============================] - 0s - loss: 0.3664 - acc: 0.8838 - val_loss: 0.5672 - val_acc: 0.7159\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "train_labels = np.array([0] * 198 + [1] * 198) #change number of labels here\n",
    "\n",
    "validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "validation_labels = np.array([0] * 132 + [1] * 132)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "model.save_weights('bottleneck_fc_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
